<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>bokeh effect generator</title>
    <link href="output.css" rel="stylesheet">
    <style>
        .click-indicator {
            position: absolute;
            width: 6px;
            height: 6px;
            background-color: red;
            border-radius: 50%;
            transform: translate(-50%, -50%);
            pointer-events: none;
            opacity: 0.5;
            z-index: 5;
        }

        .loading-overlay {
            display: none;
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: rgba(255, 255, 255, 0.9);
            justify-content: center;
            align-items: center;
            z-index: 10;
        }

        @keyframes spin {
            0% {
                transform: rotate(0deg);
            }

            100% {
                transform: rotate(360deg);
            }
        }

        .spinner {
            width: 40px;
            height: 40px;
            border: 4px solid #f3f3f3;
            border-top: 4px solid black;
            border-radius: 50%;
            animation: spin 1s linear infinite;
        }
    </style>
</head>

<body class="bg-zinc-100 min-h-screen flex flex-col items-stretch m-0 p-0">

    <div class="bg-white max-w-2xl w-full mx-auto sm:m-auto p-6">
        <h1 class="mt-0">webgpu-bokeh</h1>

        <p>
            <strong>Upload a photo, then click to set the focus. Use the sliders to adjust the blur strength. </strong>
            Runs locally in the browser thanks to WebGPU. See the <a
                href="https://github.com/fplonka/webgpu-bokeh">GitHub</a> for more details.
        </p>

        <input class="mt-2" type="file" id="imageUpload" accept="image/*">

        <div class="mt-6 relative w-full overflow-hidden flex justify-center items-center">
            <canvas id="originalCanvas" class="max-h-[65vh] max-w-full object-contain"></canvas>
            <img id="overlayImage"
                class="absolute top-0 left-0 right-0 bottom-0 max-h-[65vh] max-w-full object-contain opacity-0 pointer-events-auto"
                alt="" />
            <div id="clickIndicator" class="click-indicator" style="display:none;"></div>
            <div id="loadingOverlay" class="loading-overlay">
                <div class="spinner"></div>
            </div>
        </div>
        <div id="statusText" class="mt-4 h-6 text-zinc-500">Loading depth model...</div>

        <div class="mt-3 space-y-4">
            <div class="flex items-center gap-4">
                <label for="blurStrength" class="w-24">Blur Strength</label>
                <input type="range" id="blurStrength" min="1" max="7" step="0.1" value="5" class="flex-1" disabled>
                <input type="number" id="blurStrengthValue" min="1" max="7" step="0.1" value="5" class="w-16" disabled>
            </div>
            <div class="flex items-center gap-4">
                <label for="depthOfField" class="w-24">Depth of Field</label>
                <input type="range" id="depthOfField" min="0.01" max="0.5" step="0.01" value="0.1" class="flex-1"
                    disabled>
                <input type="number" id="depthOfFieldValue" min="0.01" max="0.5" step="0.01" value="0.1" class="w-16"
                    disabled>
            </div>
            <div class="flex items-center gap-4">
                <label for="bokehShape" class="w-24">Bokeh Shape</label>
                <select id="bokehShape" class="rounded border border-gray-300 px-2 py-1" disabled>
                    <option value="hexagon" selected>Hexagon</option>
                    <option value="octagon">Octagon</option>
                    <option value="square">Square</option>
                </select>
            </div>
            <div class="flex items-center gap-4">
                <label for="backgroundOnly" class="w-72">Only blur background (reduces artifacts)</label>
                <input type="checkbox" id="backgroundOnly" class="h-4 w-4" checked disabled>
            </div>
        </div>
    </div>

    <script type="module">
        // Check WebGPU support immediately
        if (!navigator.gpu) {
            statusText.innerHTML = "Error: WebGPU not supported. Check implementation status <a href='https://github.com/gpuweb/gpuweb/wiki/Implementation-Status'>here</a>.";
            alert('WebGPU not supported  :( try using Chrome/Edge');
            while (true) { // dont continue
                await new Promise(r => setTimeout(r, 1000));
            }
        }
        // Updates when the user uploads an image or changes setting (clicks to set focus point, changes blur strength etc)
        const state = {
            // Image and depth data
            dimensions: { width: 0, height: 0 },
            pixelTexture: null, // WebGPU texture for original pixels
            depthMap: {
                values: null, // float32 array of depth values, 0 to 1
                texture: null  // WebGPU texture with the same stuff
                // We need the values for the bokeh effect and when we click to set the focus point
            },
            // Focus and effect settings
            focus: {
                x: null,
                y: null,
                depth: null
            },
            effect: {
                blurStrength: 4,
                depthOfField: 0.1,
                shape: 'circle',
                backgroundOnly: true
            },
            // Model and rendering state
            webgpu: {
                device: null,
                context: null,
                pipelines: {
                    coc: null,
                    blurDirectional: null,
                    renderToCanvas: null,
                    combine: null
                },
                presentationFormat: null,
            },
            model: {
                estimator: null,
                loading: true
            },

            reset: function () {
                this.focus.x = null;
                this.focus.y = null;
                this.focus.depth = null;

                this.dimensions.width = 0;
                this.dimensions.height = 0;
                if (this.pixelTexture) {
                    this.pixelTexture.destroy();
                    this.pixelTexture = null;
                }
                this.depthMap.values = null;

                if (this.depthMap.texture) {
                    this.depthMap.texture.destroy();
                    this.depthMap.texture = null;
                }

                setControlsEnabled(false);
                clickIndicator.style.display = 'none';
            }
        };

        // js cringe: make state not extensible with new fields etc
        function recursiveSeal(obj) {
            Object.seal(obj);
            for (const key of Object.getOwnPropertyNames(obj)) {
                if (typeof obj[key] === 'object' && obj[key] !== null) {
                    recursiveSeal(obj[key]);
                }
            }
        }
        recursiveSeal(state);

        // Function to update the overlay image with canvas content
        function updateOverlayImage() {
            try {
                overlayImage.src = originalCanvas.toDataURL('image/png');
            } catch (error) {
                console.error('Error updating overlay image:', error);
            }
        }

        // Start loading the depth model
        import { pipeline, env } from 'https://cdn.jsdelivr.net/npm/@huggingface/transformers@3.4.2/dist/transformers.min.js';
        pipeline('depth-estimation', 'onnx-community/depth-anything-v2-base', {
            device: 'webgpu',
        }).then(model => {
            state.model.estimator = model;
            state.model.loading = false;
        }).catch(error => {
            console.error('Error loading model:', error);
            updateStatus('Error loading model. Please refresh the page.');
        });

        // UI element references
        const imageUpload = document.getElementById('imageUpload');
        const originalCanvas = document.getElementById('originalCanvas');
        const overlayImage = document.getElementById('overlayImage');
        const loadingOverlay = document.getElementById('loadingOverlay');
        const blurStrength = document.getElementById('blurStrength');
        const blurStrengthValue = document.getElementById('blurStrengthValue');
        const depthOfField = document.getElementById('depthOfField');
        const depthOfFieldValue = document.getElementById('depthOfFieldValue');
        const bokehShape = document.getElementById('bokehShape');
        const backgroundOnly = document.getElementById('backgroundOnly');
        const clickIndicator = document.getElementById('clickIndicator');

        // UI controls helper function
        function setControlsEnabled(enabled) {
            blurStrength.disabled = !enabled;
            blurStrengthValue.disabled = !enabled;
            depthOfField.disabled = !enabled;
            depthOfFieldValue.disabled = !enabled;
            bokehShape.disabled = !enabled;
            backgroundOnly.disabled = !enabled;
        }

        blurStrength.addEventListener('input', (e) => {
            blurStrengthValue.value = e.target.value;
            debouncedUpdateEffect();
        });
        blurStrengthValue.addEventListener('change', (e) => {
            blurStrength.value = e.target.value;
            debouncedUpdateEffect();
        });
        depthOfField.addEventListener('input', (e) => {
            depthOfFieldValue.value = e.target.value;
            debouncedUpdateEffect();
        });
        depthOfFieldValue.addEventListener('input', (e) => {
            depthOfField.value = e.target.value;
            debouncedUpdateEffect();
        });

        async function initWebGPU() {
            const adapter = await navigator.gpu.requestAdapter({
                featureLevel: 'compatibility'
            });
            if (!adapter) {
                throw new Error("No GPU adapter found");
            }

            // Initialize the device
            state.webgpu.device = await adapter.requestDevice({
                requiredLimits: {
                    maxStorageBufferBindingSize: adapter.limits.maxStorageBufferBindingSize,
                    maxBufferSize: adapter.limits.maxBufferSize
                }
            });

            // Configure the canvas context for WebGPU
            const canvas = originalCanvas;
            state.webgpu.context = canvas.getContext('webgpu');
            state.webgpu.presentationFormat = navigator.gpu.getPreferredCanvasFormat();
            state.webgpu.context.configure({
                device: state.webgpu.device,
                format: state.webgpu.presentationFormat,
                viewFormats: [state.webgpu.presentationFormat + '-srgb'],
                // alphaMode: 'premultiplied'
            });

            // Load and compile all shaders
            const [cocCode, blurDirectionalCode, fullscreenQuadCode, combineCode] = await Promise.all([
                fetch('coc.wgsl').then(r => r.text()),
                fetch('blur_directional.wgsl').then(r => r.text()),
                fetch('fullscreen_quad.wgsl').then(r => r.text()),
                fetch('combine.wgsl').then(r => r.text())
            ]);

            const cocShaderModule = state.webgpu.device.createShaderModule({
                code: cocCode
            });

            const blurDirectionalShaderModule = state.webgpu.device.createShaderModule({
                code: blurDirectionalCode
            });

            const fullscreenQuadModule = state.webgpu.device.createShaderModule({
                code: fullscreenQuadCode
            });

            // Create pipelines for all passes and store in state
            state.webgpu.pipelines.coc = state.webgpu.device.createComputePipeline({
                layout: 'auto',
                compute: {
                    module: cocShaderModule,
                    entryPoint: 'main'
                }
            });

            state.webgpu.pipelines.blurDirectional = state.webgpu.device.createComputePipeline({
                layout: 'auto',
                compute: {
                    module: blurDirectionalShaderModule,
                    entryPoint: 'main'
                }
            });

            // Create render pipeline for direct rendering to canvas
            state.webgpu.pipelines.renderToCanvas = state.webgpu.device.createRenderPipeline({
                layout: 'auto',
                vertex: {
                    module: fullscreenQuadModule,
                    entryPoint: 'vertex_main'
                },
                fragment: {
                    module: fullscreenQuadModule,
                    entryPoint: 'fragment_main',
                    targets: [
                        {
                            format: state.webgpu.presentationFormat + '-srgb'
                        }
                    ]
                },
                primitive: {
                    topology: 'triangle-list'
                }
            });

            // Create the combine shader module
            const combineShaderModule = state.webgpu.device.createShaderModule({
                code: combineCode
            });

            // Create the combine pipeline for blending multiple directional blurs
            state.webgpu.pipelines.combine = state.webgpu.device.createComputePipeline({
                layout: 'auto',
                compute: {
                    module: combineShaderModule,
                    entryPoint: 'main'
                }
            });
        }

        async function applyBokeh() {
            await new Promise(resolve => setTimeout(resolve, 5));
            const canvas = originalCanvas;
            const focusDepth = state.focus.depth;

            // Get the selected bokeh shape
            const selectedShape = bokehShape.value; // 'circle', 'square', or 'hexagon'
            console.log(`Applying bokeh effect with shape: ${selectedShape}`);

            // Create CoC texture
            const cocTexture = state.webgpu.device.createTexture({
                size: {
                    width: state.dimensions.width,
                    height: state.dimensions.height,
                },
                format: 'r32float',
                usage: GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.STORAGE_BINDING,
            });

            // Calculate maxCoc and number of samples
            let maxCoc = 0.001 * 0.5 * (canvas.width + canvas.height) * Math.pow(2, state.effect.blurStrength);
            const numSamples = Math.ceil(1 + maxCoc / 2);

            const shaderParamsBuffer = state.webgpu.device.createBuffer({
                size: 16, // Only need numSamples now, but keeping buffer size consistent
                usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST,
            });

            // Update effect state with current values from UI
            state.effect.shape = bokehShape.value; // Not using this directly now, but keeping for compatibility
            state.effect.blurStrength = parseFloat(blurStrength.value);
            state.effect.depthOfField = parseFloat(depthOfField.value);
            state.effect.backgroundOnly = backgroundOnly.checked;

            const shaderParamsArray = new Uint32Array([numSamples, 0, 0, 0]); // Only numSamples needed, rest is padding
            state.webgpu.device.queue.writeBuffer(shaderParamsBuffer, 0, shaderParamsArray);

            // Generate offsets for horizontal and vertical blur
            function generateOffsets(angle, numSamples) {
                const offsetsArray = new Float32Array(numSamples * 2);
                const angleRad = angle * Math.PI / 180;
                const dx = Math.cos(angleRad);
                const dy = Math.sin(angleRad);

                for (let i = 0; i < numSamples; i++) {
                    const t = (i / (numSamples - 1)) * 2.0 - 1.0;
                    offsetsArray[i * 2] = dx * t;     // x component
                    offsetsArray[i * 2 + 1] = dy * t;   // y component
                }
                return offsetsArray;
            }

            // Helper function to generate blur passes for square bokeh (horizontal + vertical)
            function processSquareBokeh(commandEncoder, cocTexture, shaderParamsBuffer, textureSampler) {
                const horizontalOffsets = generateOffsets(0, numSamples);
                const verticalOffsets = generateOffsets(90, numSamples);

                // Create buffers for offsets
                const offsetBufferSize = numSamples * 2 * 4; // 2 floats per sample, 4 bytes per float
                const horizontalOffsetsBuffer = state.webgpu.device.createBuffer({
                    size: offsetBufferSize,
                    usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST,
                });
                const verticalOffsetsBuffer = state.webgpu.device.createBuffer({
                    size: offsetBufferSize,
                    usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST,
                });

                // Write offsets to GPU
                state.webgpu.device.queue.writeBuffer(horizontalOffsetsBuffer, 0, horizontalOffsets);
                state.webgpu.device.queue.writeBuffer(verticalOffsetsBuffer, 0, verticalOffsets);

                // Create intermediate texture
                const intermediateTexture = state.webgpu.device.createTexture({
                    size: {
                        width: state.dimensions.width,
                        height: state.dimensions.height,
                    },
                    format: 'rgba8unorm',
                    usage: GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.STORAGE_BINDING,
                });

                // Create output texture (the final blurred result)
                const outputTexture = state.webgpu.device.createTexture({
                    size: {
                        width: state.dimensions.width,
                        height: state.dimensions.height,
                    },
                    format: 'rgba8unorm',
                    usage: GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.STORAGE_BINDING,
                });

                // Bind group for horizontal blur
                const blurHBindGroup = state.webgpu.device.createBindGroup({
                    layout: state.webgpu.pipelines.blurDirectional.getBindGroupLayout(0),
                    entries: [
                        { binding: 0, resource: state.pixelTexture.createView() },
                        { binding: 1, resource: intermediateTexture.createView() },
                        { binding: 2, resource: { buffer: shaderParamsBuffer } },
                        { binding: 3, resource: cocTexture.createView() },
                        { binding: 4, resource: { buffer: horizontalOffsetsBuffer } },
                        { binding: 5, resource: textureSampler },
                    ],
                });

                // Bind group for vertical blur
                const blurVBindGroup = state.webgpu.device.createBindGroup({
                    layout: state.webgpu.pipelines.blurDirectional.getBindGroupLayout(0),
                    entries: [
                        { binding: 0, resource: intermediateTexture.createView() },
                        { binding: 1, resource: outputTexture.createView() },
                        { binding: 2, resource: { buffer: shaderParamsBuffer } },
                        { binding: 3, resource: cocTexture.createView() },
                        { binding: 4, resource: { buffer: verticalOffsetsBuffer } },
                        { binding: 5, resource: textureSampler },
                    ],
                });

                // Horizontal blur pass
                const blurHPass = commandEncoder.beginComputePass();
                blurHPass.setPipeline(state.webgpu.pipelines.blurDirectional);
                blurHPass.setBindGroup(0, blurHBindGroup);
                blurHPass.dispatchWorkgroups(
                    Math.ceil(state.dimensions.width / 8),
                    Math.ceil(state.dimensions.height / 8)
                );
                blurHPass.end();

                // Vertical blur pass
                const blurVPass = commandEncoder.beginComputePass();
                blurVPass.setPipeline(state.webgpu.pipelines.blurDirectional);
                blurVPass.setBindGroup(0, blurVBindGroup);
                blurVPass.dispatchWorkgroups(
                    Math.ceil(state.dimensions.width / 8),
                    Math.ceil(state.dimensions.height / 8)
                );
                blurVPass.end();

                return outputTexture;
            }

            // Helper function to generate blur passes for hexagon bokeh (0°, 60°, 120°)
            function processHexagonBokeh(commandEncoder, cocTexture, shaderParamsBuffer, textureSampler) {
                // Generate offsets for 0°, 60°, and 120° directions
                const offsets0deg = generateOffsets(0, numSamples);
                const offsets60deg = generateOffsets(60, numSamples);
                const offsets120deg = generateOffsets(120, numSamples);

                // Create offset buffers
                const offsetBufferSize = numSamples * 2 * 4;
                const offsets0Buffer = state.webgpu.device.createBuffer({
                    size: offsetBufferSize,
                    usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST,
                });
                const offsets60Buffer = state.webgpu.device.createBuffer({
                    size: offsetBufferSize,
                    usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST,
                });
                const offsets120Buffer = state.webgpu.device.createBuffer({
                    size: offsetBufferSize,
                    usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST,
                });

                // Write offsets to GPU
                state.webgpu.device.queue.writeBuffer(offsets0Buffer, 0, offsets0deg);
                state.webgpu.device.queue.writeBuffer(offsets60Buffer, 0, offsets60deg);
                state.webgpu.device.queue.writeBuffer(offsets120Buffer, 0, offsets120deg);

                // Create textures for intermediate results
                const horizontalBlurTexture = state.webgpu.device.createTexture({
                    size: {
                        width: state.dimensions.width,
                        height: state.dimensions.height,
                    },
                    format: 'rgba8unorm',
                    usage: GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.STORAGE_BINDING,
                });

                const blur60Texture = state.webgpu.device.createTexture({
                    size: {
                        width: state.dimensions.width,
                        height: state.dimensions.height,
                    },
                    format: 'rgba8unorm',
                    usage: GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.STORAGE_BINDING,
                });

                const blur120Texture = state.webgpu.device.createTexture({
                    size: {
                        width: state.dimensions.width,
                        height: state.dimensions.height,
                    },
                    format: 'rgba8unorm',
                    usage: GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.STORAGE_BINDING,
                });

                const outputTexture = state.webgpu.device.createTexture({
                    size: {
                        width: state.dimensions.width,
                        height: state.dimensions.height,
                    },
                    format: 'rgba8unorm',
                    usage: GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.STORAGE_BINDING,
                });

                // Bind group for horizontal blur (0°)
                const blur0BindGroup = state.webgpu.device.createBindGroup({
                    layout: state.webgpu.pipelines.blurDirectional.getBindGroupLayout(0),
                    entries: [
                        { binding: 0, resource: state.pixelTexture.createView() },
                        { binding: 1, resource: horizontalBlurTexture.createView() },
                        { binding: 2, resource: { buffer: shaderParamsBuffer } },
                        { binding: 3, resource: cocTexture.createView() },
                        { binding: 4, resource: { buffer: offsets0Buffer } },
                        { binding: 5, resource: textureSampler },
                    ],
                });

                // Bind group for 60° blur
                const blur60BindGroup = state.webgpu.device.createBindGroup({
                    layout: state.webgpu.pipelines.blurDirectional.getBindGroupLayout(0),
                    entries: [
                        { binding: 0, resource: horizontalBlurTexture.createView() },
                        { binding: 1, resource: blur60Texture.createView() },
                        { binding: 2, resource: { buffer: shaderParamsBuffer } },
                        { binding: 3, resource: cocTexture.createView() },
                        { binding: 4, resource: { buffer: offsets60Buffer } },
                        { binding: 5, resource: textureSampler },
                    ],
                });

                // Bind group for 120° blur
                const blur120BindGroup = state.webgpu.device.createBindGroup({
                    layout: state.webgpu.pipelines.blurDirectional.getBindGroupLayout(0),
                    entries: [
                        { binding: 0, resource: horizontalBlurTexture.createView() },
                        { binding: 1, resource: blur120Texture.createView() },
                        { binding: 2, resource: { buffer: shaderParamsBuffer } },
                        { binding: 3, resource: cocTexture.createView() },
                        { binding: 4, resource: { buffer: offsets120Buffer } },
                        { binding: 5, resource: textureSampler },
                    ],
                });

                // Bind group for combining blur60 and blur120
                const combineBindGroup = state.webgpu.device.createBindGroup({
                    layout: state.webgpu.pipelines.combine.getBindGroupLayout(0),
                    entries: [
                        { binding: 0, resource: blur60Texture.createView() },
                        { binding: 1, resource: blur120Texture.createView() },
                        { binding: 2, resource: outputTexture.createView() },
                    ],
                });

                // Horizontal blur pass (0°)
                const blur0Pass = commandEncoder.beginComputePass();
                blur0Pass.setPipeline(state.webgpu.pipelines.blurDirectional);
                blur0Pass.setBindGroup(0, blur0BindGroup);
                blur0Pass.dispatchWorkgroups(
                    Math.ceil(state.dimensions.width / 8),
                    Math.ceil(state.dimensions.height / 8)
                );
                blur0Pass.end();

                // 60° blur pass
                const blur60Pass = commandEncoder.beginComputePass();
                blur60Pass.setPipeline(state.webgpu.pipelines.blurDirectional);
                blur60Pass.setBindGroup(0, blur60BindGroup);
                blur60Pass.dispatchWorkgroups(
                    Math.ceil(state.dimensions.width / 8),
                    Math.ceil(state.dimensions.height / 8)
                );
                blur60Pass.end();

                // 120° blur pass
                const blur120Pass = commandEncoder.beginComputePass();
                blur120Pass.setPipeline(state.webgpu.pipelines.blurDirectional);
                blur120Pass.setBindGroup(0, blur120BindGroup);
                blur120Pass.dispatchWorkgroups(
                    Math.ceil(state.dimensions.width / 8),
                    Math.ceil(state.dimensions.height / 8)
                );
                blur120Pass.end();

                // Combine pass (less bright of 60° and 120° passes)
                const combinePass = commandEncoder.beginComputePass();
                combinePass.setPipeline(state.webgpu.pipelines.combine);
                combinePass.setBindGroup(0, combineBindGroup);
                combinePass.dispatchWorkgroups(
                    Math.ceil(state.dimensions.width / 8),
                    Math.ceil(state.dimensions.height / 8)
                );
                combinePass.end();

                return outputTexture;
            }

            // Process octagon bokeh with 0°+90° and 45°+135° directional blurs
            function processOctagonBokeh(commandEncoder, cocTexture, shaderParamsBuffer, textureSampler) {
                // Generate offsets for 0°, 45°, 90°, and 135° directions
                const offsets0deg = generateOffsets(0, numSamples);
                const offsets45deg = generateOffsets(45, numSamples);
                const offsets90deg = generateOffsets(90, numSamples);
                const offsets135deg = generateOffsets(135, numSamples);

                // Create offset buffers
                const offsetBufferSize = numSamples * 2 * 4;
                const offsets0Buffer = state.webgpu.device.createBuffer({
                    size: offsetBufferSize,
                    usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST,
                });
                const offsets45Buffer = state.webgpu.device.createBuffer({
                    size: offsetBufferSize,
                    usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST,
                });
                const offsets90Buffer = state.webgpu.device.createBuffer({
                    size: offsetBufferSize,
                    usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST,
                });
                const offsets135Buffer = state.webgpu.device.createBuffer({
                    size: offsetBufferSize,
                    usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST,
                });

                // Write offsets to GPU
                state.webgpu.device.queue.writeBuffer(offsets0Buffer, 0, offsets0deg);
                state.webgpu.device.queue.writeBuffer(offsets45Buffer, 0, offsets45deg);
                state.webgpu.device.queue.writeBuffer(offsets90Buffer, 0, offsets90deg);
                state.webgpu.device.queue.writeBuffer(offsets135Buffer, 0, offsets135deg);

                // Create textures for intermediate results
                // First set of blurs: 0° -> 90°
                const horizontal0Texture = state.webgpu.device.createTexture({
                    size: {
                        width: state.dimensions.width,
                        height: state.dimensions.height,
                    },
                    format: 'rgba8unorm',
                    usage: GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.STORAGE_BINDING,
                });
                const vertical90Texture = state.webgpu.device.createTexture({
                    size: {
                        width: state.dimensions.width,
                        height: state.dimensions.height,
                    },
                    format: 'rgba8unorm',
                    usage: GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.STORAGE_BINDING,
                });

                // Second set of blurs: 45° -> 135°
                const diagonal45Texture = state.webgpu.device.createTexture({
                    size: {
                        width: state.dimensions.width,
                        height: state.dimensions.height,
                    },
                    format: 'rgba8unorm',
                    usage: GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.STORAGE_BINDING,
                });
                const diagonal135Texture = state.webgpu.device.createTexture({
                    size: {
                        width: state.dimensions.width,
                        height: state.dimensions.height,
                    },
                    format: 'rgba8unorm',
                    usage: GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.STORAGE_BINDING,
                });

                // Output texture for final result
                const outputTexture = state.webgpu.device.createTexture({
                    size: {
                        width: state.dimensions.width,
                        height: state.dimensions.height,
                    },
                    format: 'rgba8unorm',
                    usage: GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.STORAGE_BINDING,
                });

                // Bind groups for first set of blurs (0° -> 90°)
                const blur0BindGroup = state.webgpu.device.createBindGroup({
                    layout: state.webgpu.pipelines.blurDirectional.getBindGroupLayout(0),
                    entries: [
                        { binding: 0, resource: state.pixelTexture.createView() },
                        { binding: 1, resource: horizontal0Texture.createView() },
                        { binding: 2, resource: { buffer: shaderParamsBuffer } },
                        { binding: 3, resource: cocTexture.createView() },
                        { binding: 4, resource: { buffer: offsets0Buffer } },
                        { binding: 5, resource: textureSampler },
                    ],
                });
                const blur90BindGroup = state.webgpu.device.createBindGroup({
                    layout: state.webgpu.pipelines.blurDirectional.getBindGroupLayout(0),
                    entries: [
                        { binding: 0, resource: horizontal0Texture.createView() },
                        { binding: 1, resource: vertical90Texture.createView() },
                        { binding: 2, resource: { buffer: shaderParamsBuffer } },
                        { binding: 3, resource: cocTexture.createView() },
                        { binding: 4, resource: { buffer: offsets90Buffer } },
                        { binding: 5, resource: textureSampler },
                    ],
                });

                // Bind groups for second set of blurs (45° -> 135°)
                const blur45BindGroup = state.webgpu.device.createBindGroup({
                    layout: state.webgpu.pipelines.blurDirectional.getBindGroupLayout(0),
                    entries: [
                        { binding: 0, resource: state.pixelTexture.createView() },
                        { binding: 1, resource: diagonal45Texture.createView() },
                        { binding: 2, resource: { buffer: shaderParamsBuffer } },
                        { binding: 3, resource: cocTexture.createView() },
                        { binding: 4, resource: { buffer: offsets45Buffer } },
                        { binding: 5, resource: textureSampler },
                    ],
                });
                const blur135BindGroup = state.webgpu.device.createBindGroup({
                    layout: state.webgpu.pipelines.blurDirectional.getBindGroupLayout(0),
                    entries: [
                        { binding: 0, resource: diagonal45Texture.createView() },
                        { binding: 1, resource: diagonal135Texture.createView() },
                        { binding: 2, resource: { buffer: shaderParamsBuffer } },
                        { binding: 3, resource: cocTexture.createView() },
                        { binding: 4, resource: { buffer: offsets135Buffer } },
                        { binding: 5, resource: textureSampler },
                    ],
                });

                // Bind group for combining the two sets of blurs
                const combineBindGroup = state.webgpu.device.createBindGroup({
                    layout: state.webgpu.pipelines.combine.getBindGroupLayout(0),
                    entries: [
                        { binding: 0, resource: vertical90Texture.createView() },
                        { binding: 1, resource: diagonal135Texture.createView() },
                        { binding: 2, resource: outputTexture.createView() },
                    ],
                });

                // First set of passes (0° -> 90°)
                // Horizontal blur pass (0°)
                const blur0Pass = commandEncoder.beginComputePass();
                blur0Pass.setPipeline(state.webgpu.pipelines.blurDirectional);
                blur0Pass.setBindGroup(0, blur0BindGroup);
                blur0Pass.dispatchWorkgroups(
                    Math.ceil(state.dimensions.width / 8),
                    Math.ceil(state.dimensions.height / 8)
                );
                blur0Pass.end();

                // Vertical blur pass (90°)
                const blur90Pass = commandEncoder.beginComputePass();
                blur90Pass.setPipeline(state.webgpu.pipelines.blurDirectional);
                blur90Pass.setBindGroup(0, blur90BindGroup);
                blur90Pass.dispatchWorkgroups(
                    Math.ceil(state.dimensions.width / 8),
                    Math.ceil(state.dimensions.height / 8)
                );
                blur90Pass.end();

                // Second set of passes (45° -> 135°)
                // 45° diagonal blur pass
                const blur45Pass = commandEncoder.beginComputePass();
                blur45Pass.setPipeline(state.webgpu.pipelines.blurDirectional);
                blur45Pass.setBindGroup(0, blur45BindGroup);
                blur45Pass.dispatchWorkgroups(
                    Math.ceil(state.dimensions.width / 8),
                    Math.ceil(state.dimensions.height / 8)
                );
                blur45Pass.end();

                // 135° diagonal blur pass
                const blur135Pass = commandEncoder.beginComputePass();
                blur135Pass.setPipeline(state.webgpu.pipelines.blurDirectional);
                blur135Pass.setBindGroup(0, blur135BindGroup);
                blur135Pass.dispatchWorkgroups(
                    Math.ceil(state.dimensions.width / 8),
                    Math.ceil(state.dimensions.height / 8)
                );
                blur135Pass.end();

                // Combine the two blur sets by taking the less bright pixel
                const combinePass = commandEncoder.beginComputePass();
                combinePass.setPipeline(state.webgpu.pipelines.combine);
                combinePass.setBindGroup(0, combineBindGroup);
                combinePass.dispatchWorkgroups(
                    Math.ceil(state.dimensions.width / 8),
                    Math.ceil(state.dimensions.height / 8)
                );
                combinePass.end();

                return outputTexture;
            }

            // Create a sampler for texture sampling
            const textureSampler = state.webgpu.device.createSampler({
                magFilter: 'linear',
                minFilter: 'linear',
                addressModeU: 'clamp-to-edge',
                addressModeV: 'clamp-to-edge',
            });

            // Create conversion params buffer (empty as we now use texture dimensions)
            const convertParamsBuffer = state.webgpu.device.createBuffer({
                size: 16, // Just padding for consistent buffer size
                usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST,
            });
            const convertParamsArray = new Uint32Array([0, 0, 0, 0]); // Not used anymore, but keeping for compatibility
            state.webgpu.device.queue.writeBuffer(convertParamsBuffer, 0, convertParamsArray);

            // Create CoC params uniform buffer
            const cocParamsBuffer = state.webgpu.device.createBuffer({
                size: 16, // 4 floats: focus_depth, max_coc, depth_of_field, background_only
                usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST,
            });
            // Use the previously calculated maxCoc
            const cocParamsArray = new Float32Array([
                focusDepth,
                maxCoc,
                state.effect.depthOfField,
                state.effect.backgroundOnly ? 1 : 0
            ]);
            state.webgpu.device.queue.writeBuffer(cocParamsBuffer, 0, cocParamsArray);

            // Create bind groups for CoC calculation
            const cocBindGroup = state.webgpu.device.createBindGroup({
                layout: state.webgpu.pipelines.coc.getBindGroupLayout(0),
                entries: [
                    { binding: 0, resource: state.depthMap.texture.createView() },
                    { binding: 1, resource: cocTexture.createView() },
                    { binding: 2, resource: { buffer: cocParamsBuffer } },
                ],
            });

            // Create and submit command buffer for all passes
            const commandEncoder = state.webgpu.device.createCommandEncoder();

            // First pass: compute CoC values
            const cocPass = commandEncoder.beginComputePass();
            cocPass.setPipeline(state.webgpu.pipelines.coc);
            cocPass.setBindGroup(0, cocBindGroup);
            cocPass.dispatchWorkgroups(
                Math.ceil(canvas.width / 8),
                Math.ceil(canvas.height / 8)
            );
            cocPass.end();

            // Process bokeh blur based on selected shape
            let finalOutputTexture;

            if (selectedShape === 'square') {
                // Apply square bokeh (horizontal + vertical blur)
                finalOutputTexture = processSquareBokeh(commandEncoder, cocTexture, shaderParamsBuffer, textureSampler);
            } else if (selectedShape === 'hexagon') {
                // Apply hexagon bokeh (0°, 60°, 120° blur)
                finalOutputTexture = processHexagonBokeh(commandEncoder, cocTexture, shaderParamsBuffer, textureSampler);
            } else if (selectedShape === 'octagon') {
                // Apply octagon bokeh (0°+90° and 45°+135° blur)
                finalOutputTexture = processOctagonBokeh(commandEncoder, cocTexture, shaderParamsBuffer, textureSampler);
            } else {
                // Default to hexagon bokeh as a fallback
                console.log(`Using hexagon bokeh for shape: ${selectedShape}`);
                finalOutputTexture = processHexagonBokeh(commandEncoder, cocTexture, shaderParamsBuffer, textureSampler);
            }

            // Bind group for rendering final result to canvas
            const renderToCanvasBindGroup = state.webgpu.device.createBindGroup({
                layout: state.webgpu.pipelines.renderToCanvas.getBindGroupLayout(0),
                entries: [
                    { binding: 0, resource: finalOutputTexture.createView() },
                    { binding: 1, resource: textureSampler },
                ],
            });

            // Final pass: Render directly to canvas
            const renderPassEncoder = commandEncoder.beginRenderPass({
                colorAttachments: [{
                    view: state.webgpu.context.getCurrentTexture().createView({
                        format: state.webgpu.presentationFormat + '-srgb'
                    }),
                    clearValue: { r: 0.0, g: 0.0, b: 0.0, a: 1.0 },
                    loadOp: 'clear',
                    storeOp: 'store',
                }],
            });

            renderPassEncoder.setPipeline(state.webgpu.pipelines.renderToCanvas);
            renderPassEncoder.setBindGroup(0, renderToCanvasBindGroup);
            renderPassEncoder.draw(6);  // 6 vertices for fullscreen quad
            renderPassEncoder.end();

            // Submit all commands to GPU
            console.time('gpu work')
            state.webgpu.device.queue.submit([commandEncoder.finish()]);

            await state.webgpu.device.queue.onSubmittedWorkDone()
            console.timeEnd('gpu work')

            console.log("Bokeh effect applied with focus depth:", focusDepth, "max_coc:", maxCoc, "depth_of_field:", depthOfField.value);
        }

        function drawImageToCanvas(canvas, img) {
            // Set dimensions
            canvas.width = img.width;
            canvas.height = img.height;
            state.dimensions.width = canvas.width;
            state.dimensions.height = canvas.height;

            // Get pixel data via temporary canvas (required for WebGPU)
            const tempCanvas = document.createElement('canvas');
            tempCanvas.width = img.width;
            tempCanvas.height = img.height;
            const tempCtx = tempCanvas.getContext('2d');
            tempCtx.drawImage(img, 0, 0);
            const pixelData = new Uint32Array(tempCtx.getImageData(0, 0, img.width, img.height).data.buffer);

            if (state.webgpu.device) {
                // Create/update input texture
                if (state.pixelTexture) state.pixelTexture.destroy();
                state.pixelTexture = state.webgpu.device.createTexture({
                    size: {
                        width: img.width,
                        height: img.height,
                    },
                    format: 'rgba8unorm-srgb', // Use sRGB format for automatic gamma correction
                    usage: GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.COPY_DST,
                });
                state.webgpu.device.queue.writeTexture(
                    { texture: state.pixelTexture },
                    pixelData,
                    { bytesPerRow: img.width * 4 },
                    { width: img.width, height: img.height }
                );

                // Render the initial image to the canvas using WebGPU

                // Create a sampler for texture sampling
                const textureSampler = state.webgpu.device.createSampler({
                    magFilter: 'linear',
                    minFilter: 'linear',
                    addressModeU: 'clamp-to-edge',
                    addressModeV: 'clamp-to-edge',
                });

                // Create bind group for rendering to canvas
                const renderToCanvasBindGroup = state.webgpu.device.createBindGroup({
                    layout: state.webgpu.pipelines.renderToCanvas.getBindGroupLayout(0),
                    entries: [
                        { binding: 0, resource: state.pixelTexture.createView() },
                        { binding: 1, resource: textureSampler },
                    ],
                });

                // Render directly to canvas
                const commandEncoder = state.webgpu.device.createCommandEncoder();
                const renderPassEncoder = commandEncoder.beginRenderPass({
                    colorAttachments: [{
                        view: state.webgpu.context.getCurrentTexture().createView({
                            format: state.webgpu.presentationFormat + '-srgb'
                        }),
                        clearValue: { r: 0.0, g: 0.0, b: 0.0, a: 1.0 },
                        loadOp: 'clear',
                        storeOp: 'store',
                    }],
                });

                renderPassEncoder.setPipeline(state.webgpu.pipelines.renderToCanvas);
                renderPassEncoder.setBindGroup(0, renderToCanvasBindGroup);
                renderPassEncoder.draw(6);  // 6 vertices for fullscreen quad
                renderPassEncoder.end();

                // Submit command to GPU
                state.webgpu.device.queue.submit([commandEncoder.finish()]);

                state.webgpu.device.queue.onSubmittedWorkDone();
            }
        }

        // Already declared above
        const controls = document.querySelectorAll('input[type="range"], input[type="number"], input[type="checkbox"], select');



        function updateStatus(text, visible = true) {
            statusText.textContent = text;
            statusText.style.visibility = visible ? 'visible' : 'hidden';
        }

        // Debounce function to limit how often a function can be called
        function debounce(func, wait) {
            let timeout;
            return function (...args) {
                clearTimeout(timeout);
                timeout = setTimeout(() => {
                    func.apply(this, args);
                }, wait);
            };
        }

        // Add event listeners for the sliders to reapply effect when changed
        async function updateEffect() {
            if (!state.focus.x || !state.focus.y) return;

            try {
                updateStatus('Processing...');

                // Get depth at current focus point
                const index = state.focus.y * state.dimensions.width + state.focus.x;
                state.focus.depth = state.depthMap.values[index];

                await applyBokeh();
                updateStatus('Processing...', false);
            } catch (error) {
                console.error('Error applying bokeh:', error);
                updateStatus('error applying effect', true);
            }
        }

        const debouncedUpdateEffect = updateEffect;

        // For continuous slider updates - no overlay update
        blurStrength.addEventListener('input', debouncedUpdateEffect);
        depthOfField.addEventListener('input', debouncedUpdateEffect);
        
        // For slider release - update the overlay
        blurStrength.addEventListener('change', () => {
            updateEffect().then(() => updateOverlayImage());
        });
        depthOfField.addEventListener('change', () => {
            updateEffect().then(() => updateOverlayImage());
        });
        
        // For direct value changes - update the overlay
        bokehShape.addEventListener('change', () => {
            updateEffect().then(() => updateOverlayImage());
        });
        backgroundOnly.addEventListener('change', () => {
            updateEffect().then(() => updateOverlayImage());
        });

        overlayImage.addEventListener('click', async (event) => {
            if (!state.depthMap.values) return;

            updateStatus('Processing...');

            const rect = originalCanvas.getBoundingClientRect();
            const scaleX = originalCanvas.width / rect.width;
            const scaleY = originalCanvas.height / rect.height;

            // Calculate pixel coordinates from click position
            const x = Math.floor((event.clientX - rect.left) * scaleX);
            const y = Math.floor((event.clientY - rect.top) * scaleY);

            // Position dot relative to the container to handle centered canvas
            const container = originalCanvas.parentElement;
            const containerRect = container.getBoundingClientRect();

            clickIndicator.style.display = 'block';
            clickIndicator.style.left = (event.clientX - containerRect.left) + 'px';
            clickIndicator.style.top = (event.clientY - containerRect.top) + 'px';

            // Update focus state
            state.focus.x = x;
            state.focus.y = y;

            // Get depth at clicked position
            const index = y * state.depthMap.width + x;
            state.focus.depth = state.depthMap.values[index];

            console.log(`Depth at (${x}, ${y}): ${state.focus.depth.toFixed(3)}`);
            await applyBokeh();
            updateOverlayImage();
            updateStatus('Processing...', false);
            setControlsEnabled(true);
        });

        // Initialize WebGPU when the page loads
        initWebGPU().then(() => {
            // Load a default image after WebGPU is initialized
            loadDefaultImage();
        }).catch(console.error);

        // Shared function to process an image regardless of source
        async function processImage(image, statusPrefix = '') {
            try {
                state.reset(); // Reset state and clean up resources
                loadingOverlay.style.display = 'flex';
                updateStatus(`${statusPrefix}Loading image...`);

                // Draw the image on the canvas
                drawImageToCanvas(originalCanvas, image);

                // Update overlay image after drawing to canvas
                updateOverlayImage();

                // Enable click handling if model is ready
                if (!state.model.loading) {
                    originalCanvas.style.cursor = 'crosshair';
                    updateStatus('Click anywhere on the image to set focus point.');
                }

                // Wait for model if still loading
                if (state.model.loading) {
                    updateStatus('Waiting for depth model to load...');
                    while (state.model.loading) {
                        await new Promise(r => setTimeout(r, 100));
                    }
                }

                // Get depth map
                updateStatus('Estimating depth...');
                const result = await state.model.estimator(image.src);

                // Convert depth map to our format
                state.depthMap = {
                    width: state.dimensions.width,
                    height: state.dimensions.height,
                    values: new Float32Array(result.depth.data).map(v => v / 255),
                    texture: null
                };

                state.depthMap.texture = state.webgpu.device.createTexture({
                    size: {
                        width: state.dimensions.width,
                        height: state.dimensions.height,
                    },
                    format: 'r32float',
                    usage: GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.COPY_DST | GPUTextureUsage.STORAGE_BINDING,
                });

                state.webgpu.device.queue.writeTexture(
                    { texture: state.depthMap.texture },
                    state.depthMap.values,
                    { bytesPerRow: state.dimensions.width * 4 },
                    { width: state.dimensions.width, height: state.dimensions.height }
                );

                // Ready to use
                originalCanvas.style.cursor = 'crosshair';
                updateStatus('Ready! Click anywhere to set focus point.');
                loadingOverlay.style.display = 'none';
                return true;
            } catch (error) {
                console.error(`Error processing image: ${error}`);
                updateStatus(`Error processing image. ${error.message}`);
                loadingOverlay.style.display = 'none';
                return false;
            }
        }

        // Function to load a default image from a URL when the page loads
        async function loadDefaultImage() {
            // TODO:
            const defaultImageUrl = '/demo_image.jpeg';

            try {
                // Create an image element to load the default image
                const img = new Image();
                img.crossOrigin = "anonymous"; // Enable CORS for the image

                // Wait for the image to load
                await new Promise((resolve, reject) => {
                    img.onload = resolve;
                    img.onerror = reject;
                    img.src = defaultImageUrl;
                });

                // Process the image using the shared function
                await processImage(img, 'Default: ');
            } catch (error) {
                console.error('Error loading default image:', error);
                updateStatus('Error loading default image. Please upload your own image.');
            }
        }

        imageUpload.addEventListener('change', async (event) => {
            const file = event.target.files[0];
            if (!file) return;

            try {
                const image = new Image();
                image.src = URL.createObjectURL(file);

                // Wait for image to load
                await new Promise(r => image.onload = r);

                // Process the uploaded image using the shared function
                await processImage(image, 'Uploaded: ');
            } catch (error) {
                console.error('Error:', error);
                alert('Failed to process image. Please try again.');
                updateStatus('Error processing image');
                loadingOverlay.style.display = 'none';
            }
        });
    </script>
</body>

</html>