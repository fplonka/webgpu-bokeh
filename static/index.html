<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>bokeh effect generator</title>
    <link href="output.css" rel="stylesheet">
    <style>
        /* Essential styles for click indicator and loading spinner */
        .click-indicator {
            position: absolute;
            width: 6px;
            height: 6px;
            background-color: red;
            border-radius: 50%;
            transform: translate(-50%, -50%);
            pointer-events: none;
            opacity: 0.7;
        }

        .loading-overlay {
            display: none;
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            background: rgba(255, 255, 255, 0.9);
            justify-content: center;
            align-items: center;
            z-index: 10;
        }

        @keyframes spin {
            0% {
                transform: rotate(0deg);
            }

            100% {
                transform: rotate(360deg);
            }
        }

        .spinner {
            width: 40px;
            height: 40px;
            border: 4px solid #f3f3f3;
            border-top: 4px solid black;
            border-radius: 50%;
            animation: spin 1s linear infinite;
        }
    </style>
</head>

<body class="bg-zinc-100 min-h-screen p-4">
    <div class="bg-white max-w-2xl mx-auto p-6">
        <h1 class="mt-0">Bokeh Magic</h1>

        <p>
            Upload a photo, then click to set the focus. Use the sliders to adjust the blur strength.
        </p>

        <input class="mt-2" type="file" id="imageUpload" accept="image/*">

        <div class="mt-6">
            <div class="relative">
                <div class="relative">
                    <canvas id="originalCanvas" class="w-full"></canvas>
                    <div id="clickIndicator" class="click-indicator" style="display: none;"></div>
                    <div id="loadingOverlay" class="loading-overlay">
                        <div class="spinner"></div>
                    </div>
                </div>


                <div id="statusText" class="mt-2 h-6 block text-zinc-500">
                    Loading depth model, please wait...
                </div>

                <div class="mt-6 space-y-4">
                    <div class="flex items-center gap-4">
                        <label for="blurStrength" class="w-24">Blur Strength</label>
                        <!-- TODO: max 6 -->
                        <input type="range" id="blurStrength" min="1" max="7" step="0.1" value="4" class="flex-1"
                            disabled>
                        <input type="number" id="blurStrengthValue" min="1" max="7" step="0.1" value="4" class="w-16"
                            disabled>
                    </div>
                    <div class="flex items-center gap-4">
                        <label for="depthOfField" class="w-24">Depth of Field</label>
                        <input type="range" id="depthOfField" min="0.01" max="0.5" step="0.01" value="0.1"
                            class="flex-1" disabled>
                        <input type="number" id="depthOfFieldValue" min="0.01" max="0.5" step="0.01" value="0.1"
                            class="w-16" disabled>
                    </div>
                    <div class="flex items-center gap-4">
                        <label for="bokehShape" class="w-24">Bokeh Shape</label>
                        <select id="bokehShape" class="rounded border border-gray-300 px-2 py-1" disabled>
                            <option value="circle">Circle</option>
                            <option value="square">Square</option>
                            <option value="hexagon">Hexagon</option>
                        </select>
                    </div>
                    <div class="flex items-center gap-4">
                        <label for="backgroundOnly" class="w-72">Only blur background (reduces artifacts)</label>
                        <input type="checkbox" id="backgroundOnly" class="h-4 w-4" checked disabled>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <script type="module">
        // Updates when the user uploads an image or changes setting (clicks to set focus point, changes blur strength etc)
        const state = {
            // Image and depth data
            dimensions: { width: 0, height: 0 },
            pixelTexture: null, // WebGPU texture for original pixels
            depthMap: {
                values: null, // float32 array of depth values, 0 to 1
                texture: null  // WebGPU texture with the same stuff
                // We need the values for the bokeh effect and when we click to set the focus point
            },
            // Focus and effect settings
            focus: {
                x: null,
                y: null,
                depth: null
            },
            effect: {
                blurStrength: 4,
                depthOfField: 0.1,
                shape: 'circle',
                backgroundOnly: true
            },
            // Model and rendering state
            webgpu: {
                device: null,
                context: null,
                pipelines: {
                    coc: null,
                    blurDirectional: null,
                    renderToCanvas: null
                },
                presentationFormat: null,
            },
            model: {
                estimator: null,
                loading: true
            },

            reset: function () {
                this.focus.x = null;
                this.focus.y = null;
                this.focus.depth = null;

                this.dimensions.width = 0;
                this.dimensions.height = 0;
                if (this.pixelTexture) {
                    this.pixelTexture.destroy();
                    this.pixelTexture = null;
                }
                this.depthMap.values = null;

                if (this.depthMap.texture) {
                    this.depthMap.texture.destroy();
                    this.depthMap.texture = null;
                }

                setControlsEnabled(false);
                clickIndicator.style.display = 'none';
            }
        };

        // js cringe: make state not extensible with new fields etc
        function recursiveSeal(obj) {
            Object.seal(obj);
            for (const key of Object.getOwnPropertyNames(obj)) {
                if (typeof obj[key] === 'object' && obj[key] !== null) {
                    recursiveSeal(obj[key]);
                }
            }
        }
        recursiveSeal(state);

        // Start loading the depth model
        import { pipeline, env } from 'https://cdn.jsdelivr.net/npm/@huggingface/transformers@3.4.2/dist/transformers.min.js';
        pipeline('depth-estimation', 'onnx-community/depth-anything-v2-base', {
            device: 'webgpu',
        }).then(model => {
            state.model.estimator = model;
            state.model.loading = false;
            updateStatus('Click anywhere on the image to set focus point.');
        }).catch(error => {
            console.error('Error loading model:', error);
            updateStatus('Error loading model. Please refresh the page.');
        });

        // UI element references
        const imageUpload = document.getElementById('imageUpload');
        const originalCanvas = document.getElementById('originalCanvas');
        const loadingOverlay = document.getElementById('loadingOverlay');
        const blurStrength = document.getElementById('blurStrength');
        const blurStrengthValue = document.getElementById('blurStrengthValue');
        const depthOfField = document.getElementById('depthOfField');
        const depthOfFieldValue = document.getElementById('depthOfFieldValue');
        const bokehShape = document.getElementById('bokehShape');
        const backgroundOnly = document.getElementById('backgroundOnly');
        const clickIndicator = document.getElementById('clickIndicator');

        // UI controls helper function
        function setControlsEnabled(enabled) {
            blurStrength.disabled = !enabled;
            blurStrengthValue.disabled = !enabled;
            depthOfField.disabled = !enabled;
            depthOfFieldValue.disabled = !enabled;
            bokehShape.disabled = !enabled;
            backgroundOnly.disabled = !enabled;
        }

        blurStrength.addEventListener('input', (e) => {
            blurStrengthValue.value = e.target.value;
            debouncedUpdateEffect();
        });
        blurStrengthValue.addEventListener('change', (e) => {
            blurStrength.value = e.target.value;
            debouncedUpdateEffect();
        });
        depthOfField.addEventListener('input', (e) => {
            depthOfFieldValue.value = e.target.value;
            debouncedUpdateEffect();
        });
        depthOfFieldValue.addEventListener('input', (e) => {
            depthOfField.value = e.target.value;
            debouncedUpdateEffect();
        });

        async function initWebGPU() {
            if (!navigator.gpu) {
                throw new Error("WebGPU not supported");
            }

            const adapter = await navigator.gpu.requestAdapter({
                featureLevel: 'compatibility'
            });
            if (!adapter) {
                throw new Error("No GPU adapter found");
            }

            // Initialize the device
            state.webgpu.device = await adapter.requestDevice({
                requiredLimits: {
                    maxStorageBufferBindingSize: adapter.limits.maxStorageBufferBindingSize,
                    maxBufferSize: adapter.limits.maxBufferSize
                }
            });

            // Configure the canvas context for WebGPU
            const canvas = originalCanvas;
            state.webgpu.context = canvas.getContext('webgpu');
            state.webgpu.presentationFormat = navigator.gpu.getPreferredCanvasFormat();
            console.log("presentation format:", state.webgpu.presentationFormat)
            state.webgpu.context.configure({
                device: state.webgpu.device,
                format: state.webgpu.presentationFormat,
                viewFormats: [state.webgpu.presentationFormat + '-srgb'],
                // alphaMode: 'premultiplied'
            });

            // Load and compile all shaders
            const [cocCode, blurDirectionalCode, fullscreenQuadCode] = await Promise.all([
                fetch('coc.wgsl').then(r => r.text()),
                fetch('blur_directional.wgsl').then(r => r.text()),
                fetch('fullscreen_quad.wgsl').then(r => r.text())
            ]);

            const cocShaderModule = state.webgpu.device.createShaderModule({
                code: cocCode
            });

            const blurDirectionalShaderModule = state.webgpu.device.createShaderModule({
                code: blurDirectionalCode
            });

            const fullscreenQuadModule = state.webgpu.device.createShaderModule({
                code: fullscreenQuadCode
            });

            // Offset buffers are now created dynamically in applyBokeh

            // Create pipelines for all passes and store in state
            state.webgpu.pipelines.coc = state.webgpu.device.createComputePipeline({
                layout: 'auto',
                compute: {
                    module: cocShaderModule,
                    entryPoint: 'main'
                }
            });

            state.webgpu.pipelines.blurDirectional = state.webgpu.device.createComputePipeline({
                layout: 'auto',
                compute: {
                    module: blurDirectionalShaderModule,
                    entryPoint: 'main'
                }
            });

            // Create render pipeline for direct rendering to canvas
            state.webgpu.pipelines.renderToCanvas = state.webgpu.device.createRenderPipeline({
                layout: 'auto',
                vertex: {
                    module: fullscreenQuadModule,
                    entryPoint: 'vertex_main'
                },
                fragment: {
                    module: fullscreenQuadModule,
                    entryPoint: 'fragment_main',
                    targets: [
                        {
                            format: state.webgpu.presentationFormat + '-srgb'
                        }
                    ]
                },
                primitive: {
                    topology: 'triangle-list'
                }
            });

            console.log("WebGPU initialized with direct canvas rendering");
        }

        async function applyBokeh() {
            await new Promise(resolve => setTimeout(resolve, 5)); // cursed: to let the 'processing...' status update get drawn
            const canvas = originalCanvas;
            const focusDepth = state.focus.depth;

            // Create textures for processing

            // Create depth texture (source from depth map)
            const depthTexture = state.webgpu.device.createTexture({
                size: {
                    width: state.dimensions.width,
                    height: state.dimensions.height,
                },
                format: 'r32float',
                usage: GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.COPY_DST | GPUTextureUsage.STORAGE_BINDING,
            });

            // Write depth data to the texture
            state.webgpu.device.queue.writeTexture(
                { texture: depthTexture },
                state.depthMap.values,
                { bytesPerRow: state.dimensions.width * 4 },
                { width: state.dimensions.width, height: state.dimensions.height }
            );

            // Create CoC texture
            const cocTexture = state.webgpu.device.createTexture({
                size: {
                    width: state.dimensions.width,
                    height: state.dimensions.height,
                },
                format: 'r32float',
                usage: GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.STORAGE_BINDING,
            });

            // the input texture is state.pixelTexture.

            const intermediateLinearTexture = state.webgpu.device.createTexture({
                size: {
                    width: state.dimensions.width,
                    height: state.dimensions.height,
                },
                format: 'rgba8unorm',
                usage: GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.STORAGE_BINDING,
            });

            const outputLinearTexture = state.webgpu.device.createTexture({
                size: {
                    width: state.dimensions.width,
                    height: state.dimensions.height,
                },
                format: 'rgba8unorm',
                usage: GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.STORAGE_BINDING,
            });

            // Calculate maxCoc and number of samples
            let maxCoc = 0.001 * 0.5 * (canvas.width + canvas.height) * Math.pow(2, state.effect.blurStrength);
            const numSamples = Math.ceil(1 + maxCoc / 4);

            const shaderParamsBuffer = state.webgpu.device.createBuffer({
                size: 16, // Only need numSamples now, but keeping buffer size consistent
                usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST,
            });

            // Update effect state with current values from UI
            state.effect.shape = bokehShape.value; // Not using this directly now, but keeping for compatibility
            state.effect.blurStrength = parseFloat(blurStrength.value);
            state.effect.depthOfField = parseFloat(depthOfField.value);
            state.effect.backgroundOnly = backgroundOnly.checked;

            const shaderParamsArray = new Uint32Array([numSamples, 0, 0, 0]); // Only numSamples needed, rest is padding
            state.webgpu.device.queue.writeBuffer(shaderParamsBuffer, 0, shaderParamsArray);

            // Generate offsets for horizontal and vertical blur
            function generateOffsets(angle, numSamples) {
                const offsetsArray = new Float32Array(numSamples * 2);
                const angleRad = angle * Math.PI / 180;
                const dx = Math.cos(angleRad);
                const dy = Math.sin(angleRad);

                for (let i = 0; i < numSamples; i++) {
                    const t = (i / (numSamples - 1)) * 2.0 - 1.0;
                    offsetsArray[i * 2] = dx * t;     // x component
                    offsetsArray[i * 2 + 1] = dy * t;   // y component
                }
                return offsetsArray;
            }

            const horizontalOffsets = generateOffsets(0, numSamples);
            const verticalOffsets = generateOffsets(90, numSamples);

            // Create the GPU buffers for the offsets
            const offsetBufferSize = numSamples * 2 * 4; // 2 floats per sample, 4 bytes per float
            const horizontalOffsetsBuffer = state.webgpu.device.createBuffer({
                size: offsetBufferSize,
                usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST,
            });
            const verticalOffsetsBuffer = state.webgpu.device.createBuffer({
                size: offsetBufferSize,
                usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST,
            });

            // Create a sampler for texture sampling
            const textureSampler = state.webgpu.device.createSampler({
                magFilter: 'linear',
                minFilter: 'linear',
                addressModeU: 'clamp-to-edge',
                addressModeV: 'clamp-to-edge',
            });

            // Write the offsets to the GPU
            state.webgpu.device.queue.writeBuffer(horizontalOffsetsBuffer, 0, horizontalOffsets);
            state.webgpu.device.queue.writeBuffer(verticalOffsetsBuffer, 0, verticalOffsets);

            // Create conversion params buffer (empty as we now use texture dimensions)
            const convertParamsBuffer = state.webgpu.device.createBuffer({
                size: 16, // Just padding for consistent buffer size
                usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST,
            });
            const convertParamsArray = new Uint32Array([0, 0, 0, 0]); // Not used anymore, but keeping for compatibility
            state.webgpu.device.queue.writeBuffer(convertParamsBuffer, 0, convertParamsArray);

            // Create CoC params uniform buffer
            const cocParamsBuffer = state.webgpu.device.createBuffer({
                size: 16, // 4 floats: focus_depth, max_coc, depth_of_field, background_only
                usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST,
            });
            // Use the previously calculated maxCoc
            const cocParamsArray = new Float32Array([
                focusDepth,
                maxCoc,
                state.effect.depthOfField,
                state.effect.backgroundOnly ? 1 : 0
            ]);
            state.webgpu.device.queue.writeBuffer(cocParamsBuffer, 0, cocParamsArray);

            // Create bind groups for all passes
            const cocBindGroup = state.webgpu.device.createBindGroup({
                layout: state.webgpu.pipelines.coc.getBindGroupLayout(0),
                entries: [
                    { binding: 0, resource: depthTexture.createView() },
                    { binding: 1, resource: cocTexture.createView() },
                    { binding: 2, resource: { buffer: cocParamsBuffer } },
                ],
            });

            // No need for the toLinear bind group anymore since we're using rgba8unorm-srgb format

            // Bind group for horizontal (0째) blur pass in linear space
            const blurHBindGroup = state.webgpu.device.createBindGroup({
                layout: state.webgpu.pipelines.blurDirectional.getBindGroupLayout(0),
                entries: [
                    { binding: 0, resource: state.pixelTexture.createView() },
                    { binding: 1, resource: intermediateLinearTexture.createView() },
                    { binding: 2, resource: { buffer: shaderParamsBuffer } },
                    { binding: 3, resource: cocTexture.createView() },
                    { binding: 4, resource: { buffer: horizontalOffsetsBuffer } },
                    { binding: 5, resource: textureSampler },
                ],
            });

            // Bind group for vertical (90째) blur pass in linear space
            const blurVBindGroup = state.webgpu.device.createBindGroup({
                layout: state.webgpu.pipelines.blurDirectional.getBindGroupLayout(0),
                entries: [
                    { binding: 0, resource: intermediateLinearTexture.createView() },
                    { binding: 1, resource: outputLinearTexture.createView() },
                    { binding: 2, resource: { buffer: shaderParamsBuffer } },
                    { binding: 3, resource: cocTexture.createView() },
                    { binding: 4, resource: { buffer: verticalOffsetsBuffer } },
                    { binding: 5, resource: textureSampler },
                ],
            });

            // Bind group for rendering to canvas
            const renderToCanvasBindGroup = state.webgpu.device.createBindGroup({
                layout: state.webgpu.pipelines.renderToCanvas.getBindGroupLayout(0),
                entries: [
                    { binding: 0, resource: outputLinearTexture.createView() },
                    { binding: 1, resource: textureSampler }, // Reusing the same sampler we created earlier
                ],
            });

            // Create and submit command buffer for all passes
            const commandEncoder = state.webgpu.device.createCommandEncoder();

            // First pass: compute CoC values
            const cocPass = commandEncoder.beginComputePass();
            cocPass.setPipeline(state.webgpu.pipelines.coc);
            cocPass.setBindGroup(0, cocBindGroup);
            cocPass.dispatchWorkgroups(
                Math.ceil(canvas.width / 8),
                Math.ceil(canvas.height / 8)
            );
            cocPass.end();

            // Horizontal (0째) directional blur pass (in linear space)
            const blurHPass = commandEncoder.beginComputePass();
            blurHPass.setPipeline(state.webgpu.pipelines.blurDirectional);
            blurHPass.setBindGroup(0, blurHBindGroup);
            blurHPass.dispatchWorkgroups(
                Math.ceil(canvas.width / 8),
                Math.ceil(canvas.height / 8)
            );
            blurHPass.end();

            // Vertical (90째) directional blur pass (in linear space)
            const blurVPass = commandEncoder.beginComputePass();
            blurVPass.setPipeline(state.webgpu.pipelines.blurDirectional);
            blurVPass.setBindGroup(0, blurVBindGroup);
            blurVPass.dispatchWorkgroups(
                Math.ceil(canvas.width / 8),
                Math.ceil(canvas.height / 8)
            );
            blurVPass.end();

            // Final pass: Render directly to canvas
            const renderPassEncoder = commandEncoder.beginRenderPass({
                colorAttachments: [{
                    view: state.webgpu.context.getCurrentTexture().createView({
                        format: state.webgpu.presentationFormat + '-srgb'
                    }),
                    clearValue: { r: 0.0, g: 0.0, b: 0.0, a: 1.0 },
                    loadOp: 'clear',
                    storeOp: 'store',
                }],
            });

            renderPassEncoder.setPipeline(state.webgpu.pipelines.renderToCanvas);
            renderPassEncoder.setBindGroup(0, renderToCanvasBindGroup);
            renderPassEncoder.draw(6);  // 6 vertices for fullscreen quad
            renderPassEncoder.end();

            // Submit all commands to GPU
            console.time('gpu work')
            state.webgpu.device.queue.submit([commandEncoder.finish()]);

            await state.webgpu.device.queue.onSubmittedWorkDone()
            console.timeEnd('gpu work')

            console.log("Bokeh effect applied with focus depth:", focusDepth, "max_coc:", maxCoc, "depth_of_field:", depthOfField.value);
        }

        function drawImageToCanvas(canvas, img) {
            // Set dimensions
            canvas.width = img.width;
            canvas.height = img.height;
            state.dimensions.width = canvas.width;
            state.dimensions.height = canvas.height;

            // Get pixel data via temporary canvas (required for WebGPU)
            const tempCanvas = document.createElement('canvas');
            tempCanvas.width = img.width;
            tempCanvas.height = img.height;
            const tempCtx = tempCanvas.getContext('2d');
            tempCtx.drawImage(img, 0, 0);
            const pixelData = new Uint32Array(tempCtx.getImageData(0, 0, img.width, img.height).data.buffer);

            if (state.webgpu.device) {
                // Create/update input texture
                if (state.pixelTexture) state.pixelTexture.destroy();
                state.pixelTexture = state.webgpu.device.createTexture({
                    size: {
                        width: img.width,
                        height: img.height,
                    },
                    format: 'rgba8unorm-srgb', // Use sRGB format for automatic gamma correction
                    usage: GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.COPY_DST,
                });
                state.webgpu.device.queue.writeTexture(
                    { texture: state.pixelTexture },
                    pixelData,
                    { bytesPerRow: img.width * 4 },
                    { width: img.width, height: img.height }
                );

                // TODO: don't draw the image on load, need to fix textures etc bla bla
                return;

                // Create uniform buffer for dimensions
                const paramsBuffer = state.webgpu.device.createBuffer({
                    size: 16,
                    usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST,
                });
                state.webgpu.device.queue.writeBuffer(paramsBuffer, 0, new Uint32Array([img.width, img.height, 0, 0]));

                // Create linear color space texture
                const linearTexture = state.webgpu.device.createTexture({
                    size: {
                        width: img.width,
                        height: img.height,
                    },
                    format: 'rgba8unorm',
                    usage: GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.STORAGE_BINDING,
                });

                // Convert to linear space
                // (this is a bit dumb, we use it to reuse the same rendering shader we use when applying the blur)
                const computeEncoder = state.webgpu.device.createCommandEncoder();
                const computePass = computeEncoder.beginComputePass();
                computePass.setPipeline(state.webgpu.pipelines.toLinear);
                computePass.setBindGroup(0, state.webgpu.device.createBindGroup({
                    layout: state.webgpu.pipelines.toLinear.getBindGroupLayout(0),
                    entries: [
                        { binding: 0, resource: state.pixelTexture.createView() },
                        { binding: 1, resource: linearTexture.createView() },
                        { binding: 2, resource: { buffer: paramsBuffer } },
                    ],
                }));
                computePass.dispatchWorkgroups(Math.ceil(img.width / 8), Math.ceil(img.height / 8));
                computePass.end();
                state.webgpu.device.queue.submit([computeEncoder.finish()]);

                // Render to canvas
                const renderEncoder = state.webgpu.device.createCommandEncoder();
                const renderPass = renderEncoder.beginRenderPass({
                    colorAttachments: [{
                        view: state.webgpu.context.getCurrentTexture().createView(),
                        clearValue: { r: 0.0, g: 0.0, b: 0.0, a: 1.0 },
                        loadOp: 'clear',
                        storeOp: 'store',
                    }],
                });
                renderPass.setPipeline(state.webgpu.pipelines.renderToCanvas);
                renderPass.setBindGroup(0, state.webgpu.device.createBindGroup({
                    layout: state.webgpu.pipelines.renderToCanvas.getBindGroupLayout(0),
                    entries: [
                        { binding: 0, resource: linearTexture.createView() },
                        { binding: 1, resource: textureSampler }, // Add sampler here too
                    ],
                }));
                renderPass.draw(6);
                renderPass.end();
                state.webgpu.device.queue.submit([renderEncoder.finish()]);

                state.webgpu.device.queue.onSubmittedWorkDone();
            }
        }

        // Already declared above
        const controls = document.querySelectorAll('input[type="range"], input[type="number"], input[type="checkbox"], select');

        function updateStatus(text, visible = true) {
            statusText.textContent = text;
            statusText.style.visibility = visible ? 'visible' : 'hidden';
        }

        // Debounce function to limit how often a function can be called
        function debounce(func, wait) {
            let timeout;
            return function (...args) {
                clearTimeout(timeout);
                timeout = setTimeout(() => {
                    func.apply(this, args);
                }, wait);
            };
        }

        // Add event listeners for the sliders to reapply effect when changed
        async function updateEffect() {
            if (!state.focus.x || !state.focus.y) return;

            try {
                updateStatus('processing...');

                // Get depth at current focus point
                const index = state.focus.y * state.dimensions.width + state.focus.x;
                state.focus.depth = state.depthMap.values[index];

                await applyBokeh();
                updateStatus('processing...', false);
            } catch (error) {
                console.error('Error applying bokeh:', error);
                updateStatus('error applying effect', true);
            }
        }

        const debouncedUpdateEffect = debounce(updateEffect, 20);

        // Use the debounced function for continuous input events
        blurStrength.addEventListener('input', debouncedUpdateEffect);
        depthOfField.addEventListener('input', debouncedUpdateEffect);
        bokehShape.addEventListener('change', updateEffect);
        backgroundOnly.addEventListener('change', updateEffect);

        originalCanvas.addEventListener('click', async (event) => {
            if (!state.depthMap.values) return;

            updateStatus('processing...');

            const rect = originalCanvas.getBoundingClientRect();
            const scaleX = originalCanvas.width / rect.width;
            const scaleY = originalCanvas.height / rect.height;

            // Calculate pixel coordinates from click position
            const x = Math.floor((event.clientX - rect.left) * scaleX);
            const y = Math.floor((event.clientY - rect.top) * scaleY);

            // Update click indicator position
            clickIndicator.style.display = 'block';
            clickIndicator.style.left = (event.clientX - rect.left) + 'px';
            clickIndicator.style.top = (event.clientY - rect.top) + 'px';

            // Update focus state
            state.focus.x = x;
            state.focus.y = y;

            // Get depth at clicked position
            const index = y * state.depthMap.width + x;
            state.focus.depth = state.depthMap.values[index];

            console.log(`Depth at (${x}, ${y}): ${state.focus.depth.toFixed(3)}`);
            await applyBokeh();
            updateStatus('processing...', false);
            setControlsEnabled(true);
        });

        // Initialize WebGPU when the page loads
        initWebGPU().catch(console.error);

        imageUpload.addEventListener('change', async (event) => {
            const file = event.target.files[0];
            if (!file) return;

            state.reset(); // Reset state and clean up resources
            loadingOverlay.style.display = 'flex';
            updateStatus('Loading image...');

            try {
                const image = new Image();
                image.src = URL.createObjectURL(file);

                // Wait for image to load
                await new Promise(r => image.onload = r);
                drawImageToCanvas(originalCanvas, image);

                // Enable click handling if model is ready
                if (!state.model.loading) {
                    originalCanvas.style.cursor = 'crosshair';
                    updateStatus('Click anywhere on the image to set focus point.');
                }

                // Wait for model if still loading
                if (state.model.loading) {
                    while (state.model.loading) {
                        await new Promise(r => setTimeout(r, 100));
                    }
                }

                // Get depth map
                updateStatus('Estimating depth...');
                const result = await state.model.estimator(image.src);

                // Convert depth map to our format
                state.depthMap = {
                    width: state.dimensions.width,
                    height: state.dimensions.height,
                    values: new Float32Array(result.depth.data).map(v => v / 255),
                    texture: null
                };

                // Create the depth texture once when we first get the depth map
                if (state.webgpu.device && state.depthMap.values) {
                    // Create a single reusable depth texture
                    state.depthMap.texture = state.webgpu.device.createTexture({
                        size: {
                            width: state.dimensions.width,
                            height: state.dimensions.height,
                        },
                        format: 'r32float',
                        usage: GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.COPY_DST | GPUTextureUsage.STORAGE_BINDING,
                    });
                    state.webgpu.device.queue.writeTexture(
                        { texture: state.depthMap.texture },
                        state.depthMap.values,
                        { bytesPerRow: state.dimensions.width * 4 },
                        { width: state.dimensions.width, height: state.dimensions.height }
                    );
                }



                updateStatus('Ready! Click anywhere to set focus point.');
            } catch (error) {
                console.error('Error:', error);
                alert('Failed to process image. Please try again.');
                updateStatus('Error processing image');
            } finally {
                loadingOverlay.style.display = 'none';
            }
        });
    </script>
</body>

</html>